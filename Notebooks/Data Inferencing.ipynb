{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide path to inference data\n",
    "\n",
    "df = pd.read_csv(\"../Inference Data/Kickstarter_inference_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_date(column):\n",
    "    date_list = []\n",
    "    start_date = datetime(year=1970, month=1, day=1)\n",
    "    for r in column:\n",
    "        new_date = start_date + timedelta(seconds = int(r))\n",
    "        date_list.append(new_date)\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function - check input if valid JSON before reading it in\n",
    "# In try except, be specific about the error\n",
    "\n",
    "def parse_json(column, keyword):\n",
    "    new_column = []\n",
    "    for r in column:\n",
    "        try:\n",
    "            new_column.append(json.loads(r)[keyword])\n",
    "        except:\n",
    "            new_column.append(None)\n",
    "    return new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.created_at = clean_date(df.created_at)\n",
    "df.state_changed_at = clean_date(df.state_changed_at)\n",
    "df.deadline = clean_date(df.deadline)\n",
    "df.launched_at = clean_date(df.launched_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.category = parse_json(df.category, 'name')\n",
    "df.creator = parse_json(df.creator, 'name')\n",
    "df['location_city'] = parse_json(df.location, 'localized_name')\n",
    "df['location_state'] = parse_json(df.location, 'state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drops duplicate rows that match on id, deadline, and creator\n",
    "df = df.drop_duplicates(subset=['id', 'deadline', 'creator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop json Array columns\n",
    "df = df.drop(columns=['location','photo','profile','urls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index().drop(columns='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to projects only with successful or failed state for training purposes\n",
    "df = df[((df.state == 'successful') | (df.state == 'failed'))]\n",
    "\n",
    "# Drop columns without distinct values (found from profiling report)\n",
    "df = df.drop(columns=['disable_communication','friends','is_backing','is_starred','permissions'])\n",
    "\n",
    "# Drop highly correlated columns (ex. currency and currency symbol)\n",
    "df = df.drop(columns=['country_displayable_name','currency_symbol','currency_trailing_code','static_usd_rate','usd_exchange_rate','usd_type'])\n",
    "\n",
    "# Drop columns that are not known at the beginning of a project\n",
    "df = df.drop(columns=['backers_count','converted_pledged_amount','pledged','spotlight','state_changed_at','usd_pledged'])\n",
    "\n",
    "# Drop columns irrelevant due to business knowledge\n",
    "df = df.drop(columns=['currency','current_currency','fx_rate','slug','source_url','location_city'])\n",
    "\n",
    "# Drop columns due to difficulty to model (potentially NLP future additions)\n",
    "df = df.drop(columns=['blurb','creator','name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['campaign_length'] = (df['deadline'] - df['launched_at']).dt.days\n",
    "df['prep_time'] = (df['launched_at'] - df['created_at']).dt.days\n",
    "df['month_of_launch'] = df['launched_at'].dt.month\n",
    "df['weekday_of_launch'] = df['launched_at'].dt.dayofweek\n",
    "df['hour_of_launch'] = df['launched_at'].dt.hour\n",
    "\n",
    "# Drop date columns due to difficulty for interpretation for ML\n",
    "df = df.drop(columns=['deadline','created_at','launched_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-01 03:03:19,845 featuretools - WARNING    Featuretools failed to load \"nlp_primitives\" primitives from \"nlp_primitives\". For a full stack trace, set logging to debug.\n",
      "2023-02-01 03:03:19,873 featuretools - WARNING    Featuretools failed to load plugin nlp_primitives from library nlp_primitives. For a full stack trace, set logging to debug.\n"
     ]
    }
   ],
   "source": [
    "# Encode Data\n",
    "from evalml.pipelines.components.transformers import OneHotEncoder\n",
    "\n",
    "number_of_categories = 10\n",
    "ohe = OneHotEncoder(top_n=number_of_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df, column_to_encode):\n",
    "    ohe.fit(df[column_to_encode])\n",
    "    return ohe.transform(df[column_to_encode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns outside top_n receive an other value for encoding purposes\n",
    "columns_to_encode = ['location_state','country','category','staff_pick', 'is_starrable']\n",
    "\n",
    "for column in columns_to_encode:\n",
    "    top_cats = df[column].value_counts().head(number_of_categories).index.tolist()\n",
    "    df.loc[~df[column].isin(top_cats), column] = 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_df = encode(df, columns_to_encode)\n",
    "df = df.drop(columns=columns_to_encode)\n",
    "encoded_df['id'] = df['id']\n",
    "inference_df = pd.merge(df, encoded_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df = inference_df.drop(columns=['state'])\n",
    "inference_df = inference_df.drop(columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_df.to_json(\"../Tables/inference_data.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
